---
title: "Project 3"
author: "Yvette Callender"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: TRUE
        toc_depth: 2
params: 
    EducationLevel: "Elementary"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The following libraries were used in this project.

```{r, message = FALSE}
library(tidyverse)
library(caret)
```

# Introduction

In this project, data related to diabetes health was analyzed using a variety of models.  

## Data 
A Diabetes Health Indicators Dataset: `diabetes_binary_health_indicators_BRFSS2015.csv` was used for this project.  This dataset can be found online [on Kaggle.](https://www.kaggle.com/code/jerryodegua/eda-prediction-of-diabetes-data) along with descriptions of the variabless.

## Variables
The dataset contains 21 variables than can potentially be used in the analysis.  A subset was chosen and they are described below.

The variable `diabetes_binary` is the response variable for this analysis. It is either a 0 (corresponding to a lack of either prediabetes or diabetes) or a 1 (corresponding to the presence of either prediabetes or diabetes).

The variable `HighBP` is a binary variable with 1 corresponding to high blood pressure and 0 corresponding to normal blood pressure.

The variable `HighChol` is a binary variable with 1 corresponding to high cholesterol levels and 0 corresponding to normal cholesterol levels.

The variable `BMI` (Body Mass Index) is a numeric variable.  As part of the analysis, it was converted to a categorical variable with values corresponding to underweight (BMI $\le$ 18.5), normal(18.5 $\le$ BMI $\le$ 24.9), overweight(25.0 $\le$ BMI $\le$ 29.9) and obese(30 $\le$ BMI).

The variable `HeartDiseaseorAttack` is a binary variable where 1 indicates a history of heart disease or attack and a 0 indicates a lack thereof.

The variable `GenHlth` is a categorical variable describing the the reported level of general health as described below:

|Level|Description|
|:---|:-----------:|
|1|excellent|
|2|very good|
|3|good|
|4|fair|
|5|poor|


The variable `DiffWalk` is binary with 1 corresponding to reported difficulty walking or climbing stairs and 0 corresponding to no reported 

The variable `Age` is a categorical variable with the following levels.  

|Level|Description|
|:---|:-----------:|
|1|18-24 years old|
|2|25-29 years old|
|3|30-34 years old|
|4|35-39 years old|
|5|40-44 years old|
|6|45-54 years old|
|7|50-54 years old|
|8|55-59 years old|
|9|60-64 years old|
|10|65-69 years old|
|11|70-74 years old|
|12|75-79 years old|
|13|$\ge$ 80 years old|


The variable `Education` is a categorical variable with the following levels. 

|Education Level|Description|
|:---|:-----------:|
|1|Never attended school or only kindergarten|
|2|Grades 1 through 8 (Elementary)|
|3|Grades 9-11 (Some high school)|
|4|Grade 12 or GED (High school graduate)|
|5|College 1 year to 3 years (Some college or technical school)|
|6|College 4 years or more (College graduate)|

For this analysis, data was divided into subsets based on the value of `Education`.  There are five subsets: one for levels one and two combined, and one each for levels three, four, five, and six.

## Purpose of EDA and Modeling and End Results

The relationship between the presence of diabetes/prediabetes (as indicated by `diabetes_binary`) and the predictor variables (`HighBP`, `HighChol`, `BMI` , `HeartDiseaseorAttack`, `GenHlth`, `DiffWalk`, and `Age`) were explored at several levels of education.  After Exploratory Data Analysis, the relationships were modeled using a variety or approaches (logistic regression, LASSO logistic regression, classification tree, random forest, *other 1, and other 2*) and a best model was chosen for each level of education. 

# Data Set-Up

Data was imported and converted to a tibble.  Columns corresponding to categorical variables were converted to factors.

```{r, cache=TRUE}

#Read in Data
diabetes_data <- read.csv ("diabetes_binary_health_indicators_BRFSS2015.csv") %>% 
                 as_tibble


#Select Desired Columns
diabetes_data <- diabetes_data %>%
    select(Diabetes_binary, HighBP, HighChol, BMI, HeartDiseaseorAttack, GenHlth,
           DiffWalk, Age, Education)

#For education make a new column where 1 and 2 are combined
diabetes_data$EducationDerived <- ifelse(diabetes_data$Education == 1 |
                                         diabetes_data$Education == 2 , '1_and_2',
                                         diabetes_data$Education)

#Convert EducationDerived to factor with labels
diabetes_data$EducationDerived <- factor(diabetes_data$EducationDerived, 
                                            levels = c('1_and_2', '3','4','5','6'), 
                                            labels = c("Elementary", "Some High School",
                                                       "High School Graduate", 
                                                       "Some College or Technical School",
                                                       "College Graduate"))

#Convert of binary variable to factors
diabetes_data <- diabetes_data %>% 
    mutate(Diabetes_binary = factor(Diabetes_binary, levels = c("0","1"), 
        labels = c("Non-diabetic", "Diabetic"))) %>%
    mutate(HighBP = factor(HighBP, levels = c("0","1"), 
        labels = c("Normal BP", "High BP"))) %>%
    mutate(HighChol = factor(HighChol, levels = c("0","1"), 
        labels = c("Normal Cholesterol", "High Cholesterol"))) %>%
    mutate(HeartDiseaseorAttack = factor(HeartDiseaseorAttack, levels = c("0","1"), 
        labels = c("No History", "History of Heart Disease or Attack"))) %>%
    mutate(DiffWalk = factor(DiffWalk, levels = c("0","1"), 
        labels = c("No Difficulty Walking or Climbing Stairs", 
                   "Difficulty Walking or Climbing Stairs"))) 

#Convert BMI to ordered factors
diabetes_data <- diabetes_data %>% 
    mutate(BMIFactor = if_else (BMI <= 18.5, "Underweight",
                                if_else (BMI <= 24.9, "Healthy",
                                    if_else (BMI <= 29.9, "Overweight", "Obese"))))
diabetes_data$BMIFactor <- ordered(diabetes_data$BMIFactor, 
                                   levels = c("Underweight","Healthy","Overweight","Obese"))

#Convert age to factors 
diabetes_data <- diabetes_data %>% 
    mutate(Age = factor(Age, 
        levels =  c("1","2","3","4","5","6","7","8","9","10", "11", "12", "13"), 
                 labels = c("18-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54",
                          "55-59", "60-64", "65-69", "70-74", "75-79", ">= 80" ))) 

# Convert GenHlth to factors
diabetes_data <- diabetes_data %>% 
    mutate(GenHlth = factor(GenHlth, 
        levels =  c("1","2","3","4","5"), 
                 labels = c("Excellent", "Very good", "Good", "Fair", "Poor")))
```

Data was filtered for the desired education level.

```{r}
diabetes_data_subset <- diabetes_data %>% 
    filter(EducationDerived == params$EducationLevel)
knitr::kable(head(diabetes_data_subset, n=10))
```


# Summarizations for Education = `r params$EducationLevel`

## Response Variable: `Diabetes_binary`

The  amounts of nondiabetics and diabetics in this dataset are:

```{r, Table-1}
#Make and print table
table1 <- table(diabetes_data_subset$Diabetes_binary)
knitr::kable(table1, col.names = c("Diabetes Status", "Frequency"))
```

`r paste0("There are ",  round(table1[1]/table1[2],1), "X as many nondiabetics as diabetics in the dataset at Education = ",  params$EducationLevel, ".")`

This can be seen graphically here.

```{r, Figure-1}
#Base layer
figure1 <- ggplot(diabetes_data_subset, aes(x = Diabetes_binary))
#Build up
figure1 + geom_bar() +
    labs( x = "Diabetes Status",
          y = "Count",
          title = "Figure 1. Diabetes Status Distribution")
```

## Predictor variable: `HighBP`

The distribution in this dataset for high blood pressure is:

```{r, Table-2}
#Make and print table
table2 <- table(diabetes_data_subset$HighBP)
knitr::kable(table2, col.names = c("Blood Pressure", "Frequency"))
```

The distribution for diabetes status in this dataset as a function of blood pressure status is:
```{r, Table-3}
#Make and print table
table3 <- table(diabetes_data_subset$Diabetes_binary,diabetes_data_subset$HighBP)
knitr::kable(table3)
```

This can be seen graphically here.

```{r, Figure-2}
#Base layer
figure2 <- ggplot(diabetes_data_subset, aes(x = Diabetes_binary), )
#Build up
figure2 + geom_bar(aes(fill = HighBP)) +
    labs( x = "Diabetes Status",
          y = "Count",
          title = "Figure 2. Diabetes Status Distribution versus Blood Pressure Status") +
    guides(fill = guide_legend(title = "Blood Pressure"))
```

## Predictor variable: `HighChol`
The distribution in this dataset for high cholesterol is:

```{r, Table-4}
#Make and print table
table4 <- table(diabetes_data_subset$HighChol)
knitr::kable(table4, col.names = c("Cholesterol", "Frequency"))
```

The distribution for diabetes status in this dataset as a function of cholesterol status is:
```{r, Table-5}
#Make and print table
table5 <- table(diabetes_data_subset$Diabetes_binary,diabetes_data_subset$HighChol)
knitr::kable(table5)
```

This can be seen graphically here.

```{r, Figure-3}
#Base layer
figure3 <- ggplot(diabetes_data_subset, aes(x = Diabetes_binary), )
#Build up
figure3 + geom_bar(aes(fill = HighChol)) +
    labs( x = "Diabetes Status",
          y = "Count",
          title = "Figure 3. Diabetes Status Distribution versus Cholesterol Status") +
    guides(fill = guide_legend(title = "Cholesterol"))
```

## Predictor variable: `BMI`
The distribution in this dataset for BMI as a numeric value is:

```{r, Table-6}
#Make and print table
table6 <- diabetes_data_subset %>% 
    summarize (Variable ="BMI",
               Minimum = min(BMI),
               Median= median(BMI),
               Maximum=max(BMI),
               Mean = round(mean (BMI),1),
               StdDev=round(sd(BMI),1)
              )
knitr::kable(table6)

```

This can be visualized as a boxplot:

```{r, Figure 4}
figure4<-ggplot() +
         geom_boxplot(aes (y=diabetes_data_subset$BMI))+
         labs (y= "BMI",
               title= "Figure 4. Boxplot for BMI")+
         theme(axis.title.x = element_blank(),
               axis.text.x = element_blank(),
               axis.ticks.x = element_blank())
figure4
```

Based on [Center for Diseased Control  guidelines](https://www.cdc.gov/healthyweight/assessing/bmi/adult_bmi/index.html), the variable BMIFactor was set to underweight, healthy, overweight, or obese.  The distribution with regard to BMI at Education = `r params$EducationLevel` is shown here. 

```{r, Figure-5}
#Base layer
figure5 <- ggplot(diabetes_data_subset, aes(x = BMIFactor)) 
#Build up
figure5 + geom_bar() +
    labs( x = "BMI Classification",
          y = "Count",
          title = "Figure 5. BMI Classification Distribution")
```

The effect of BMI of the relative amounts of nondiabetics versus diabeticsis shown here.

```{r, Figure-6}
#Base layer
figure6 <- ggplot(diabetes_data_subset, aes(x = BMIFactor), )
#Build up
figure6 + geom_bar(aes(fill = Diabetes_binary), position= "dodge") +
    labs( x = "BMI Classification",
          y = "Count",
          title = "Figure 6. Diabetes Status Distribution versus BMI ") +
    guides(fill = guide_legend(title = "Diabetes Status"))

```

## Predictor variable: `HeartDiseaseorAttack`
The distribution in this dataset for a history of heart disease or attack is:

```{r, Table-7}
#Make and print table
table7 <- table(diabetes_data_subset$HeartDiseaseorAttack)
knitr::kable(table7, col.names = c("History of Heart Trouble", "Frequency"))
```

The distribution for diabetes status in this dataset as a function of whether the subject has had a heart diseased or attack is:

```{r, Table-8}
#Make and print table
table8 <- table(diabetes_data_subset$Diabetes_binary,diabetes_data_subset$HeartDiseaseorAttack)
knitr::kable(table8)
```

This can be seen graphically here.

```{r, Figure-7}
#Base layer
figure7 <- ggplot(diabetes_data_subset, aes(x = Diabetes_binary), )
#Build up
figure7 + geom_bar(aes(fill = HeartDiseaseorAttack)) +
    labs( x = "Diabetes Status",
          y = "Count",
          title = "Figure 7. Diabetes Status Distribution versus History of Heart Disease or Attack") +
    guides(fill = guide_legend(title = "History of Heart Trouble"))
```


## Predictor variable: `GenHlth`
The distribution in this dataset for a history of heart disease or attack is:

```{r, Table-9}
#Make and print table
table9 <- table(diabetes_data_subset$GenHlth)
knitr::kable(table9, col.names = c("Subject Description of General Health", "Frequency"))
```

The distribution for diabetes status in this dataset as a function of whether the subject has had a heart diseased or attack is:

```{r, Table-10}
#Make and print table
table10 <- table(diabetes_data_subset$Diabetes_binary,diabetes_data_subset$GenHlth)
knitr::kable(table10)
```

This can be seen graphically here.

```{r, Figure-8}
#Base layer
figure8 <- ggplot(diabetes_data_subset, aes(x = GenHlth), )
#Build up
figure8 + geom_bar(aes(fill = Diabetes_binary), position= "dodge") +
    labs( x = "General Health of Subject",
          y = "Count",
          title = "Figure 8. Diabetes Status Distribution versus General Health")+
    guides(fill = guide_legend(title = "Diabetes Status"))
```

## Predictor variable: `DiffWalk`
The distribution in this dataset for a history of heart disease or attack is:

```{r, Table-11}
#Make and print table
table11 <- table(diabetes_data_subset$DiffWalk)
knitr::kable(table11, col.names = c(" ", "Frequency"))
```

The distribution for diabetes status in this dataset as a function of whether the subject has had a heart diseased or attack is:

```{r, Table-12}
#Make and print table
table12 <- table(diabetes_data_subset$Diabetes_binary,diabetes_data_subset$DiffWalk)
knitr::kable(table12)
```

This can be seen graphically here.

```{r, Figure-9}
#Base layer
figure9 <- ggplot(diabetes_data_subset, aes(x = Diabetes_binary), )
#Build up
figure9 + geom_bar(aes(fill = DiffWalk), position="dodge") +
    labs( x = "Diabetes Status",
          y = "Count",
          title = "Figure 9. Diabetes Status Distribution versus Difficulty Walking or Climbing Stairs") +
      guides(fill = guide_legend(title = " "))
```

## Predictor variable: `Age`
The distribution in this dataset for age is:
```{r, Table-13}
#Make and print table
table13 <- table(diabetes_data_subset$Age) 
knitr::kable(table13, col.names = c("Age Range ", "Frequency"))
```

The distribution for diabetes status in this dataset as a function of age is:

```{r, Table-14}
#Make and print table
table14 <- table(diabetes_data_subset$Diabetes_binary,diabetes_data_subset$Age)
knitr::kable(table14)
```

This can be seen graphically here.

```{r, Figure-10}
#Base layer
figure10 <- ggplot(diabetes_data_subset, aes(x = Age))
#Build up
figure10 + geom_bar(aes(fill = Diabetes_binary), position="dodge") +
    labs( x = "Age",
          y = "Count",
          title = "Figure 10. Diabetes Status Distribution versus Age") +
    guides(fill = guide_legend(title = "Diabetes Status "))
```

# Modeling

## Split Data Into Training and Test

The `createDataPartition` from `caret` was used to partition the data into training (70% of the data) and test (30% of the data) sets. Diabetics and non-diabetics were split separately.

```{r}
#for reproducibility
set.seed(1331)
#generate indices for split 
trainIndex <-createDataPartition(diabetes_data_subset$Diabetes_binary, p =0.7, list=FALSE)

#split data
diabetes_data_subset_train <- diabetes_data_subset[trainIndex,]
diabetes_data_subset_test <- diabetes_data_subset[-trainIndex,]

```

## Log Loss Function
[Reference](https://www.youtube.com/watch?v=MztgenIfGgM)  

Logarithmic loss (or log loss or cross-entropy loss) is a performance measure for a binary classification model which outputs a probability between 0 and 1.  Values for log loss can range from 0 to $\infty$ infinity, with 0 representing a perfect model.  The equation for determining Log Loss is:
$$ LogLoss = -\frac{1}{N}\sum_{i=1}^{N} (y_ilog(p(y_i))+(1-y_i)log (1-p(y_i)) $$
It has a desirable feature of being convex and having a single global minimum. This is in contrast to the MSE (mean square error) function used in linear regression which is not convex and can have many local minima, which makes it unsuitable for use in logistic regression.  

When accuracy is used as a performance measure, it only takes right and wrong into account; whereas the log loss function has weighting terms that take into account just how wrong the model is.  While accuracy would treat probabilities of 0.05 and 0.45 for a true "1" as equally wrong, log loss would impose a much higher penalty (3.8 X greater) on the confident but wrong p=0.05.  

Log loss analysis can be incorporated into `caret` by setting the `metric = logloss`.

## Logistic Regression
[Reference](https://www.youtube.com/watch?v=MztgenIfGgM)  

Logistic Regression is a method used on dataset where the response (dependent) variable is binary. The response variable is fit as a logistic sigmoid function of independent variables which can be continuous or binary.  The general form of the equation is $$ y= \frac{1}{1+e^{-X}}$$
where  X is a vector containing all the predictor variables.  The range of this function is 0-1 which works well with a binary dependent variable.  
The logistic function is linked to the X vector with the logit function.
$$log\frac{p}{1-p}=\beta_0 + \beta_1x_1 + \beta_2x_2+ ...+ \beta_px_p$$

### Model 1 
### Training


### Model 2 
### Training


### Model 3 
### Training


## LASSO Logistic Regression
[Reference](https://www.mygreatlearning.com/blog/understanding-of-lasso-regression/)  

Least Absolute Shrinkage and Selection Operator (LASSO) is a penalized method for modelling. It is used in an attempt to balance accuracy and simplicity.  The penalty (assuming n predictor variables) is calculated as $$L_1 = \lambda * (|\beta_1| + |\beta_2| + ... + |\beta_n|)$$

$\lambda$ is a tuning parameter.  Larger values of $\lambda$ push more coefficients to zero leading to sparser models, while smaller values of $\lambda$ allow more non-zero coefficients, corresponding to more complex models. 

During the modelling the function that is minimized- referred to as the objective function- is the sum of the penalty function and the cost function (which for this logistic regression will be log loss). $$Objective Function = Log Loss + L_1 = Log Loss + \lambda * (|\beta_1| + |\beta_2| + ... + |\beta_n|)$$


## Training


## Classification Tree Model
Classification trees involve splitting the predictor space into regions, and predictions are made based on the regions, usually the most prevalent class is used as the prediction.  Strengths of classifications trees include ease of understanding and interpretability, the fact that predictors do  no require scaling, no statistical assumptions are necessary, and variable selection is built in.  Weaknesses include sensitivity to small changes in the data, a need for a greedy algorithm, and a need for pruning.  



Vary complexity factor


### Training


## Random Forest Model
[Reference](https://www.ibm.com/topics/random-forest)  

Random forest is a tree-based method of modeling.  It uses multiple decision trees to arrive at a single results.  It uses bootstrap/ aggregating  (bagging) and feature randomness to "create an uncorrelated forest of decision trees".  It is the feature randomness that distinguishes the random forest model from the classification tree.  There are three parameters that need to be set in the random forest model: node size, number of trees, and number of features sampled.  

While classification trees are prone to overfit the data (making it conform too tightly to the training set), that risk is reduced in a random forest model due to the averaging of results from uncorrelated trees.  Another advantage of random forest over classification trees is that it is easier to evaluate the importance of different predictor variables.

### Training


## Ridge Logistic Regression

[Reference](https://www.cvxpy.org/examples/machine_learning/ridge_regression.html)  

Ridge logistic regression is a penalized method for modelling. It is used in an attempt to balance accuracy and simplicity.  The penalty (assuming n predictor variables) is calculated as $$L_2 = \frac{\lambda}{2} * (\beta_1^2 + \beta_2^2 + ... + \beta_n^2)$$

$\lambda$ is a complexity parameter.  Larger values of $\lambda$ push more coefficients to zero while smaller values of $\lambda$ allow more coefficients to remain larger.  In ridge regression, coefficients will never be pushed to zero.

During the modelling the function that is minimized- referred to as the objective function- is the sum of the penalty function and the cost function (which for this logistic regression will be log loss). $$Objective Function = Log Loss + L_2 = Log Loss + \frac{\lambda}{2} * (\beta_1^2 + \beta_2^2 + ... + \beta_n^2)$$

Because $L_2$ does not include an intercept term, it is necessary to standardize predictor variables appropriately. 

### Training



## Elastic net Logistic Regression

[Reference](https://machinelearningmastery.com/elastic-net-regression-in-python/)

Elastic net regression is a penalized method for modelling.  Its penalty is equal to a combination of the L~1~  penalty (based on the absolute value of the coefficients of the predictors) from the LASSO method and the L~2~ penalty (based on the square of the coefficients of the predictors) from the ridge method.  

For elastic net, the parameter $\alpha$ controls the balance between the L~1~ and L~2~ penalties. $$ElasticNetPenalty = \alpha*L_1 + (1-\alpha)L_2 =$$

$\alpha$ can range from 0 to 1.  When $\alpha = 0$, elastic net is equivalent to ridge; when $\alpha = 1$, elastic net is equivalent to LASSO.

LASSO regression suffers from instability when predictors are collinear, arbitrarily selecting one predictor over another. Ridge regression may keep too many predictors in a similar situation.  Elastic net can strike a balance between the other two.  

While the LASSO and Ridge methods each only have one tuning parameter (called $\lambda$), elastic net requires a two tuning parameters ($\alpha$) and $\lambda$.  Having two tuning parameters makes use of elastic net more time-consuming and computationally expensive then the LASSO and ridge methods.


### Training
pkg elasticnet
enet
fraction
lambda

# Final Model Selection